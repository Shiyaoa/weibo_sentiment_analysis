{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:49:24.039185Z",
     "start_time": "2020-12-14T02:49:21.980739Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn import manifold,metrics\n",
    "import jieba\n",
    "#import wandb\n",
    "#wandb.init(project=\"attention\")\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:49:24.044185Z",
     "start_time": "2020-12-14T02:49:24.040182Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wvmodel(mode=\"word\"):\n",
    "    if mode==\"word\":\n",
    "        wvmodel=gensim.models.Word2Vec.load(\"weibomodel\").wv\n",
    "    else:\n",
    "        wvmodel=gensim.models.Word2Vec.load(\"weibo_zh.model\").wv\n",
    "    return wvmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:49:28.538044Z",
     "start_time": "2020-12-14T02:49:28.506036Z"
    }
   },
   "outputs": [],
   "source": [
    "traindata=pd.read_csv(\"./dataset/train.txt\",header=None,sep='\\t')\n",
    "traindata.columns=['review','label']\n",
    "validdata=pd.read_csv(\"./dataset/dev.txt\",header=None,sep='\\t')\n",
    "validdata.columns=['review','label']\n",
    "testdata=pd.read_csv(\"./dataset/test.txt\",header=None,sep='\\t')\n",
    "testdata.columns=['review','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:49:43.730932Z",
     "start_time": "2020-12-14T02:49:43.724930Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX=20\n",
    "\n",
    "\n",
    "def encode_samples(tokenized_samples):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features\n",
    "\n",
    "def preprocess_data(dataframe):\n",
    "    data=dataframe.copy()\n",
    "    data['cut']=data['review'].apply(lambda x:jieba.lcut(x))\n",
    "    return data[['label','cut']]\n",
    "\n",
    "def load_data(data,MAX,batch_size,mode=\"train\"):\n",
    "    x_data=data['cut'].values.tolist()\n",
    "    y_data=data['label'].values.tolist()\n",
    "    _features = torch.tensor(pad_samples(encode_samples(x_data),maxlen=MAX))\n",
    "    _labels = torch.tensor(y_data,dtype=torch.int64)\n",
    "    _set = torch.utils.data.TensorDataset(_features, _labels)\n",
    "    if mode==\"train\":\n",
    "        _iter = torch.utils.data.DataLoader(_set, batch_size=batch_size,\n",
    "                                             shuffle=True)\n",
    "    else:\n",
    "        _iter = torch.utils.data.DataLoader(_set, batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "    return _iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:07.663943Z",
     "start_time": "2020-12-14T02:56:05.069375Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_data=preprocess_data(traindata)\n",
    "train_iter=load_data(train_data,MAX,batch_size=BATCH_SIZE,mode=\"train\")\n",
    "valid_data=preprocess_data(validdata)\n",
    "valid_iter=load_data(valid_data,MAX,batch_size=BATCH_SIZE,mode=\"valid\")\n",
    "test_data=preprocess_data(testdata)\n",
    "test_iter=load_data(test_data,MAX,batch_size=BATCH_SIZE,mode=\"test\")\n",
    "\n",
    "x_data=pd.concat([train_data.cut,valid_data.cut,test_data.cut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:10.939293Z",
     "start_time": "2020-12-14T02:56:10.916287Z"
    }
   },
   "outputs": [],
   "source": [
    "#词表长度\n",
    "vocab = set(chain(*x_data))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:11.704463Z",
     "start_time": "2020-12-14T02:56:11.684458Z"
    }
   },
   "outputs": [],
   "source": [
    "#切词长度\n",
    "len(list(chain(*x_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:12.780709Z",
     "start_time": "2020-12-14T02:56:12.564655Z"
    }
   },
   "outputs": [],
   "source": [
    "wvmodel=get_wvmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:13.205795Z",
     "start_time": "2020-12-14T02:56:13.187802Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:14.125009Z",
     "start_time": "2020-12-14T02:56:13.868952Z"
    }
   },
   "outputs": [],
   "source": [
    "weight = torch.zeros(vocab_size+1, embed_size)\n",
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index2word[i]]\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n",
    "        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**here we use self-attention**\n",
    "$$ U=tanh\\left(WX\\right)$$\n",
    "$$ \\alpha_{t} = softmax(u_{t}u_{w}) $$\n",
    "$$ ScoreX=X\\alpha $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:22.013757Z",
     "start_time": "2020-12-14T02:56:22.005769Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.weight_W = nn.Parameter(torch.rand(num_hiddens*2, num_hiddens*2))\n",
    "            self.weight_proj = nn.Parameter(torch.rand(num_hiddens*2, 1))\n",
    "        else:\n",
    "            self.weight_W = nn.Parameter(torch.rand(num_hiddens, num_hiddens))\n",
    "            self.weight_proj = nn.Parameter(torch.rand(num_hiddens, 1))\n",
    "            \n",
    "        self.decoder=nn.Sequential(nn.Linear(num_hiddens*2,num_hiddens),\n",
    "            nn.Dropout(p=0.5),nn.ReLU(),nn.Linear(num_hiddens,labels))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "\n",
    "        #embeddings=[batch_size,seq_length,embedded_size]\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        #states=[seq_len, batch, num_directions * hidden_size]\n",
    "        x=states.permute([1,0,2])\n",
    "        #x=[batch,seq_len,num_directions*hidden_szie]\n",
    "        \n",
    "        #####attention calculation#####\n",
    "        u = torch.tanh(torch.matmul(x, self.weight_W))\n",
    "        #u=[batch,seq_len,num_directions*hidden_szie]\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        #att=[batch,seq_len,1]\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        #att_score=[batch,seq_len,1],which sum by {seq_len}=1\n",
    "        scored_x = x * att_score\n",
    "        #scored_x=[batch,seq_len,num_directions*hidden_szie]\n",
    "        #####attention_weighted_x\n",
    "        \n",
    "        scored_x = torch.sum(scored_x, dim=1)\n",
    "        #scored_x=[batch,num_directions*hidden_szie]\n",
    "        outputs=self.decoder(scored_x)\n",
    "        #outputs=[batch,labels]\n",
    "        \n",
    "        return outputs,embeddings,att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:23.714150Z",
     "start_time": "2020-12-14T02:56:23.708142Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len, labels, weight, **kwargs):\n",
    "        super(textCNN, self).__init__(**kwargs)\n",
    "        self.labels = labels\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.conv1 = nn.Conv2d(1, 1, (2, embed_size))\n",
    "        self.conv2 = nn.Conv2d(1, 1, (3, embed_size))\n",
    "        self.conv3 = nn.Conv2d(1, 1, (4, embed_size))\n",
    "        self.pool1 = nn.MaxPool2d((seq_len - 2 + 1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((seq_len - 3 + 1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((seq_len - 4 + 1, 1))\n",
    "        self.linear = nn.Linear(3, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs).view(inputs.shape[0], 1, inputs.shape[1], -1)\n",
    "        x1 = F.relu(self.conv1(inputs))\n",
    "        x2 = F.relu(self.conv2(inputs))\n",
    "        x3 = F.relu(self.conv3(inputs))\n",
    "\n",
    "        x1 = self.pool1(x1)\n",
    "        x2 = self.pool2(x2)\n",
    "        x3 = self.pool3(x3)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), -1)\n",
    "        x = x.view(inputs.shape[0], 1, -1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, self.labels)\n",
    "\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:56:24.123228Z",
     "start_time": "2020-12-14T02:56:24.116233Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def category_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.max(preds,1)[1]\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        vectors,labels=batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions,embeddings,att = model(vectors)\n",
    "        #print(att[0,:,:])\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = category_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator),epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            vectors,labels=batch\n",
    "\n",
    "            predictions,embeddings,att = model(vectors)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = category_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:57:14.143418Z",
     "start_time": "2020-12-14T02:57:14.132424Z"
    }
   },
   "outputs": [],
   "source": [
    "##如果是字向量，改embed_size\n",
    "embed_size = 100\n",
    "num_hiddens = 64\n",
    "num_layers = 1\n",
    "labels=7\n",
    "bidirectional = True\n",
    "lr = 0.001\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True\n",
    "model = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional, weight=weight,\n",
    "                   labels=labels, use_gpu=use_gpu)\n",
    "model.to(device)\n",
    "#wandb.watch(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:57:22.765344Z",
     "start_time": "2020-12-14T02:57:19.012518Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "trainacc,val_acc,trainloss,val_loss=[],[],[],[]\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss,train_acc= train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss,valid_acc = evaluate(model, test_iter, criterion)\n",
    "        \n",
    "    end = time.time()\n",
    "        \n",
    "    runtime = end - start\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm-model.pt')\n",
    "    trainacc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    trainloss.append(train_loss)\n",
    "    val_loss.append(valid_loss)\n",
    "\n",
    "    #wandb.log({\"Test Accuracy\": valid_acc, \"Test Loss\": valid_loss})\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.4f, test loss: %.4f, test acc: %.4f, time: %.2f' %\n",
    "      (epoch, train_loss, train_acc, valid_loss,valid_acc, runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:32:31.842940Z",
     "start_time": "2020-12-08T18:32:31.354830Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 绘制训练 \n",
    "plt.plot(trainacc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 绘制训练 & 验证的损失值\n",
    "plt.plot(trainloss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:33:05.452504Z",
     "start_time": "2020-12-08T18:33:04.579322Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feature(model, iterator,labels):\n",
    "    model.eval()\n",
    "    labels=labels\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        origin=torch.zeros([1,MAX*embed_size],dtype=torch.float)\n",
    "        output=torch.zeros([1,labels],dtype=torch.float)\n",
    "        label=torch.zeros([1],dtype=torch.int64)\n",
    "        tmp_origin=origin\n",
    "        tmp_output=output\n",
    "        for batch in iterator:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            vectors,label_list=batch\n",
    "            predictions,embeddings,att = model(vectors)\n",
    "            embedding=embeddings.view(embeddings.size()[0],embeddings.size()[1]*embeddings.size()[2])\n",
    "            tmp_output=torch.cat((tmp_output,predictions.cpu().data),0)\n",
    "            tmp_origin=torch.cat((tmp_origin,embedding.cpu().data),0)\n",
    "            #print(label_list.size())\n",
    "            label=torch.cat((label,label_list.cpu().data),0)\n",
    "    return tmp_origin,tmp_output,label\n",
    "\n",
    "model.load_state_dict(torch.load('lstm-model.pt'))\n",
    "begin,end,label=get_feature(model,test_iter,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:35:17.383265Z",
     "start_time": "2020-12-08T18:35:17.379265Z"
    }
   },
   "outputs": [],
   "source": [
    "X=end.numpy()[1:,]\n",
    "\n",
    "Y=label.numpy()[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:36:18.908560Z",
     "start_time": "2020-12-08T18:35:23.754719Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=501)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T18:36:26.964376Z",
     "start_time": "2020-12-08T18:36:18.910561Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "X_norm = (X_tsne - x_min) / (x_max - x_min)  # 归一化\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.text(X_norm[i, 0], X_norm[i, 1], str(Y[i]), color=plt.cm.Set1(Y[i]), \n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
