{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:09.127542Z",
     "start_time": "2020-12-14T02:28:06.671989Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel,BertConfig\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn import metrics\n",
    "from bert_SourceCode.optimization import BertAdam\n",
    "import time \n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:10.461820Z",
     "start_time": "2020-12-14T02:28:09.128523Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('chinese-bert-wwm-ext')\n",
    "bert = BertModel.from_pretrained('chinese-bert-wwm-ext')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:13.144479Z",
     "start_time": "2020-12-14T02:28:13.111413Z"
    }
   },
   "outputs": [],
   "source": [
    "traindata=pd.read_csv(\"./dataset/train.txt\",header=None,sep='\\t')\n",
    "traindata.columns=['review','label']\n",
    "validdata=pd.read_csv(\"./dataset/dev.txt\",header=None,sep='\\t')\n",
    "validdata.columns=['review','label']\n",
    "testdata=pd.read_csv(\"./dataset/test.txt\",header=None,sep='\\t')\n",
    "testdata.columns=['review','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:21.815371Z",
     "start_time": "2020-12-14T02:28:21.809368Z"
    }
   },
   "outputs": [],
   "source": [
    "#截长去短\n",
    "def pad(sentlist,maxlen,PAD=0):\n",
    "    padded_list = sentlist\n",
    "    while(len(padded_list) < maxlen):\n",
    "        padded_list.append(PAD)\n",
    "    return padded_list\n",
    "\n",
    "#对pad元素进行mask\n",
    "def mask(sentlist):\n",
    "    attention_mask=[float(i>0) for i in sentlist]\n",
    "    return attention_mask\n",
    "\n",
    "#数据预处理，需要增加首尾标记、同长、转id、增加mask\n",
    "def preprocess_data(dataframe,MAX=20):\n",
    "    data=dataframe.copy()\n",
    "    data['review']=data['review'].apply(lambda x:x[:MAX])\n",
    "    data['preprocess']=['[CLS] ' + sent + ' [SEP]' for sent in data['review'].values]\n",
    "    data['tokenized']=[tokenizer.tokenize(sent) for sent in data['preprocess']]\n",
    "    data['original_inputs_id']=[tokenizer.convert_tokens_to_ids(sent) for sent in data['tokenized']]\n",
    "    data['inputs_id']=data['original_inputs_id'].apply(pad,maxlen=MAX+2)\n",
    "    data['attention_mask']=data['inputs_id'].apply(mask)\n",
    "    return data[['inputs_id','label','attention_mask']]\n",
    "\n",
    "#把数据转换成tensor类型，并加载入dataloader生成迭代器\n",
    "def load_data(data,batch_size,mode=\"train\"):\n",
    "    inputs=torch.tensor(data['inputs_id'].tolist(),dtype=torch.int64)\n",
    "    labels=torch.tensor(data['label'].tolist(),dtype=torch.int64)\n",
    "    masks=torch.tensor(data['attention_mask'].tolist(),dtype=torch.float)\n",
    "    if mode==\"train\":\n",
    "        _data = torch.utils.data.TensorDataset(inputs, masks, labels)\n",
    "        _iter = torch.utils.data.DataLoader(_data, shuffle=True, batch_size=batch_size)\n",
    "    else:\n",
    "        _data = torch.utils.data.TensorDataset(inputs, masks, labels)\n",
    "        _iter = torch.utils.data.DataLoader(_data, shuffle=False, batch_size=batch_size)\n",
    "    return _iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:29.558103Z",
     "start_time": "2020-12-14T02:28:26.392384Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_iter=load_data(preprocess_data(traindata),batch_size=BATCH_SIZE)\n",
    "valid_iter=load_data(preprocess_data(validdata),batch_size=BATCH_SIZE,mode='eva')\n",
    "test_iter=load_data(preprocess_data(testdata),batch_size=BATCH_SIZE,mode='eva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:33.261929Z",
     "start_time": "2020-12-14T02:28:33.255911Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BERTFCSentiment(nn.Module):\n",
    "    def __init__(self,bert,output_dim):\n",
    "        \n",
    "        super(BERTFCSentiment,self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.bertconfig=bert.config\n",
    "        self.bertconfig.output_attentions=True\n",
    "        self.bertconfig.output_hidden_states=True\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.decoder=nn.Sequential(nn.Linear(embedding_dim,embedding_dim//2),\n",
    "            nn.Dropout(p=0.5),nn.ReLU(),nn.Linear(embedding_dim//2,output_dim))\n",
    "            \n",
    "        \n",
    "\n",
    "        self.weight_W = nn.Parameter(torch.rand(embedding_dim,embedding_dim))\n",
    "        self.weight_proj = nn.Parameter(torch.rand(embedding_dim, 1))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        #inputs_ids = [batch size, sent len]\n",
    "        input_ids,input_mask,_=batch\n",
    "        bert_out=self.bert(input_ids=input_ids,attention_mask=input_mask)\n",
    "        last_hidden_state = bert_out[0]\n",
    "        pooled=bert_out[1]\n",
    "\n",
    "        \n",
    "        ####below is a kind of self attention\n",
    "        u = torch.tanh(torch.matmul(last_hidden_state, self.weight_W))\n",
    "        # u=[batch_size, sequence_length, hidden_size]\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        # att=[batch_size, sequence_length,1]\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        # att_score=[batch_size, sequence_length,1],which sum by {seq_len}=1\n",
    "        scored_x = last_hidden_state * att_score\n",
    "        # scored_x=[batch_size, sequence_length,hidden_size]\n",
    "        #####attention_weighted_x\n",
    "\n",
    "        scored_x = torch.sum(scored_x, dim=1)\n",
    "        output=self.decoder(scored_x)\n",
    "        \n",
    "        #above is attention for last_hidden_state,\n",
    "        #if using pooled for decoder,just annotate above code\n",
    "        # output = self.decoder(pooled)\n",
    "        \n",
    "\n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output,att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:34.919294Z",
     "start_time": "2020-12-14T02:28:34.912288Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BERTLSTMSentimentNet(nn.Module):\n",
    "    def __init__(self, bert,output_dim,num_hiddens,bidirectional, **kwargs):\n",
    "        super(BERTLSTMSentimentNet, self).__init__(**kwargs)\n",
    "\n",
    "        self.bert = bert\n",
    "        self.bertconfig=bert.config\n",
    "        self.bertconfig.output_attentions=True\n",
    "        self.bertconfig.output_hidden_states=True\n",
    "        self.embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size=self.embedding_dim,\n",
    "                               hidden_size=self.num_hiddens,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.weight_W = nn.Parameter(torch.rand(self.num_hiddens*2, self.num_hiddens*2))\n",
    "            self.weight_proj = nn.Parameter(torch.rand(self.num_hiddens*2, 1))\n",
    "        else:\n",
    "            self.weight_W = nn.Parameter(torch.rand(self.num_hiddens, self.num_hiddens))\n",
    "            self.weight_proj = nn.Parameter(torch.rand(self.num_hiddens,1))\n",
    "            \n",
    "        self.decoder=nn.Sequential(nn.Linear(self.num_hiddens*2,self.num_hiddens),\n",
    "            nn.Dropout(p=0.5),nn.ReLU(),nn.Linear(self.num_hiddens,output_dim))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids,input_mask,_=batch\n",
    "        bert_out=self.bert(input_ids=input_ids,attention_mask=input_mask)\n",
    "        last_hidden_state = bert_out[0]\n",
    "        #embeddings=[batch_size,seq_length,embedded_size]\n",
    "        states, hidden = self.encoder(last_hidden_state.permute([1, 0, 2]))\n",
    "        #states=[seq_len, batch, num_directions * hidden_size]\n",
    "        x=states.permute([1,0,2])\n",
    "        #x=[batch,seq_len,num_directions*hidden_szie]\n",
    "        \n",
    "        #####attention calculation#####\n",
    "        u = torch.tanh(torch.matmul(x, self.weight_W))\n",
    "        #u=[batch,seq_len,num_directions*hidden_szie]\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        #att=[batch,seq_len,1]\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        #att_score=[batch,seq_len,1],which sum by {seq_len}=1\n",
    "        scored_x = x * att_score\n",
    "        #scored_x=[batch,seq_len,num_directions*hidden_szie]\n",
    "        #####attention_weighted_x\n",
    "        \n",
    "        scored_x = torch.sum(scored_x, dim=1)\n",
    "        #scored_x=[batch,num_directions*hidden_szie]\n",
    "        outputs=self.decoder(scored_x)\n",
    "        #outputs=[batch,labels]\n",
    "        \n",
    "        return outputs,att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:35.511412Z",
     "start_time": "2020-12-14T02:28:35.504411Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTATSentiment(nn.Module):\n",
    "    def __init__(self,bert,output_dim):\n",
    "        \n",
    "        super(BERTATSentiment,self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.bertconfig=bert.config\n",
    "        self.bertconfig.output_hidden_states=True\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.decoder=nn.Sequential(nn.Linear(embedding_dim,embedding_dim//2),\n",
    "            nn.Dropout(p=0.5),nn.ReLU(),nn.Linear(embedding_dim//2,output_dim))\n",
    "            \n",
    "        \n",
    " \n",
    "        self.weight_W = nn.Parameter(torch.rand(embedding_dim,embedding_dim))\n",
    "        self.weight_proj = nn.Parameter(torch.rand(embedding_dim, 1))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        #inputs_ids = [batch size, sent len]\n",
    "        input_ids,input_mask,_=batch\n",
    "        bert_out=self.bert(input_ids=input_ids,attention_mask=input_mask)\n",
    "        last_hidden_state = bert_out[0]\n",
    "        pooled=bert_out[1]\n",
    "        all_hidden_states=bert_out[2]\n",
    "        \n",
    "        batch_size = input_ids.shape[0]\n",
    "        seq_length= input_ids.shape[1]\n",
    "        #an easy idea:just concatnate by seq_length\n",
    "        x = torch.cat(all_hidden_states,1)\n",
    "        #x=[batch_size,13*seq_length,hidden_seize]\n",
    "        \n",
    "         #####attention calculation#####\n",
    "        u = torch.tanh(torch.matmul(x, self.weight_W))\n",
    "        #u=[batch,seq_len,num_directions*hidden_szie]\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        #att=[batch,seq_len,1]\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        #att_score=[batch,seq_len,1],which sum by {seq_len}=1\n",
    "        scored_x = x * att_score\n",
    "        #scored_x=[batch,seq_len,num_directions*hidden_szie]\n",
    "        #####attention_weighted_x\n",
    "        \n",
    "        scored_x = torch.sum(scored_x, dim=1)\n",
    "        #scored_x=[batch,num_directions*hidden_szie]\n",
    "        outputs=self.decoder(scored_x)\n",
    "        #outputs=[batch,labels]\n",
    "\n",
    "\n",
    "        \n",
    "        return outputs,att_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:41.392734Z",
     "start_time": "2020-12-14T02:28:41.384738Z"
    }
   },
   "outputs": [],
   "source": [
    "#这是bertfc的参数设置\n",
    "N_EPOCHS = 2\n",
    "OUTPUT_DIM = 7\n",
    "device = torch.device('cuda:0')\n",
    "lr = 5e-5\n",
    "model = BERTFCSentiment(bert,OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:04:46.165097Z",
     "start_time": "2020-12-14T02:04:46.162102Z"
    }
   },
   "outputs": [],
   "source": [
    "#这是bertlstm的参数设置\n",
    "#N_EPOCHS = 50\n",
    "#OUTPUT_DIM = 7\n",
    "#NUM_HIDDENS=64\n",
    "#bidirectional=True\n",
    "#device = torch.device('cuda:0')\n",
    "#lr = 5e-5\n",
    "#model=BERTLSTMSentimentNet(bert,OUTPUT_DIM,NUM_HIDDENS,bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:13:07.353865Z",
     "start_time": "2020-12-14T02:13:07.345872Z"
    }
   },
   "outputs": [],
   "source": [
    "#这是bertat的参数设置\n",
    "#N_EPOCHS = 2\n",
    "#OUTPUT_DIM = 7\n",
    "#device = torch.device('cuda:0')\n",
    "#lr = 5e-5\n",
    "#model = BERTATSentiment(bert,OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:04:46.645454Z",
     "start_time": "2020-12-14T02:04:46.642460Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#for name, param in model.named_parameters():                \n",
    "#    if name.startswith('bert'):\n",
    "#        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:48.919339Z",
     "start_time": "2020-12-14T02:28:48.913342Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "#解决不平衡问题，改用focalloss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "        This criterion is a implemenation of Focal Loss, which is proposed in \n",
    "        Focal Loss for Dense Object Detection.\n",
    "\n",
    "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "\n",
    "        The losses are averaged across observations for each minibatch.\n",
    "\n",
    "        Args:\n",
    "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
    "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), \n",
    "                                   putting more focus on hard, misclassiﬁed examples\n",
    "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
    "                                However, if the field size_average is set to False, the losses are\n",
    "                                instead summed for each minibatch.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs)\n",
    "\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "        #print(class_mask)\n",
    "\n",
    "\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "\n",
    "        probs = (P*class_mask).sum(1).view(-1,1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "        #print('probs size= {}'.format(probs.size()))\n",
    "        #print(probs)\n",
    "\n",
    "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n",
    "        #print('-----bacth_loss------')\n",
    "        #print(batch_loss)\n",
    "\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:51.124819Z",
     "start_time": "2020-12-14T02:28:49.647505Z"
    }
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "network_param = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in network_param)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in network_param)], 'weight_decay': 0.0}]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=lr,\n",
    "                         warmup=0.05,\n",
    "                         t_total=len(train_iter) * N_EPOCHS)\n",
    "#optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "alpha=Variable(torch.tensor([0.13,0.2,0.12,0.13,0.14,0.14,0.14]))\n",
    "criterion=FocalLoss(class_num=OUTPUT_DIM,alpha=alpha)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:51.128821Z",
     "start_time": "2020-12-14T02:28:51.125820Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def category_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.max(preds,1)[1]\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:51.691955Z",
     "start_time": "2020-12-14T02:28:51.687961Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids,input_mask,labels=batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions,attention = model(batch)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = category_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator),epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:52.355095Z",
     "start_time": "2020-12-14T02:28:52.352093Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids,input_mask,labels=batch\n",
    "\n",
    "            predictions,attention = model(batch)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = category_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:28:53.702402Z",
     "start_time": "2020-12-14T02:28:53.696409Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class_list = [x.strip() for x in open( 'dataset/class.txt').readlines()]\n",
    "\n",
    "def test(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    predicts_all = np.array([], dtype=int)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids,input_mask,labels=batch\n",
    "\n",
    "            predictions,attention = model(batch)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            acc = category_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predicts = torch.max(predictions, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predicts_all = np.append(predicts_all, predicts)\n",
    "            \n",
    "    report = metrics.classification_report(labels_all, predicts_all, target_names=class_list, digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predicts_all)\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator),report,confusion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T02:31:35.623560Z",
     "start_time": "2020-12-14T02:29:09.712985Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "trainacc,val_acc,trainloss,val_loss=[],[],[],[]\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss,train_acc= train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss,valid_acc = evaluate(model, valid_iter, criterion)\n",
    "        \n",
    "    end = time.time()\n",
    "        \n",
    "    runtime = end - start\n",
    "    #模型保不保存无所谓了 做实验为主    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bertlstm-model.pt')\n",
    "    trainacc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    trainloss.append(train_loss)\n",
    "    val_loss.append(valid_loss)\n",
    "\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.4f, test loss: %.4f, test acc: %.4f, time: %.2f' %\n",
    "      (epoch, train_loss, train_acc, valid_loss,valid_acc, runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:09.125659Z",
     "start_time": "2020-12-13T19:56:08.649547Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 绘制训练 \n",
    "plt.plot(trainacc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 绘制训练 & 验证的损失值\n",
    "plt.plot(trainloss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:11.115090Z",
     "start_time": "2020-12-13T19:56:09.126646Z"
    }
   },
   "outputs": [],
   "source": [
    "result=test(model,test_iter, criterion)\n",
    "print(\"test_acc:  %.4f ,test loss: %.4f\"%(result[1],result[0]))\n",
    "print(result[2])\n",
    "print(result[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
